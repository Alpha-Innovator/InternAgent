{
    "name": "HEDGE-Net",
    "title": "HEDGE-Net: Hierarchical Equivariant Directional Graph Encoder for Molecular Energy and Force Prediction",
    "description": "HEDGE-Net introduces a refined SE(3)-equivariant graph neural network for molecular energy and force prediction, focusing on hierarchical geometric aggregation and improved anisotropic message passing. Leveraging a Geometry-Enhanced Directional Attention (GEDA) mechanism, it directly integrates angular and directional features into aggregated substructures, while ensuring SE(3)-equivariance throughout the pipeline. The method enables effective learning across both atomic and substructural scales, preserving scalability and precision for complex molecular systems.",
    "statement": "The innovative contributions of HEDGE-Net include: (1) a Geometry-Enhanced Directional Attention (GEDA) mechanism that explicitly incorporates directional and angular features into hierarchical self-attention updates, achieving precise modeling of local substructural interactions, and (2) a unified framework that provides provable SE(3)-equivariance throughout message passing, attention computation, and hierarchical aggregation. By addressing limitations of existing methods regarding incomplete equivariant guarantees and unclear integration of angular features, HEDGE-Net enhances expressivity and scalability for large-scale molecular systems. This represents a significant advancement in geometric deep learning for molecular property prediction.",
    "method": "### Notation and Definitions\n1. **Molecular Graph Representation**: A molecule is represented as a graph \\( G = (V, E) \\):\n   - \\( V \\) represents atoms \\( \\{v_i: i = 1, 2, \\dots, |V|\\} \\), where each \\( v_i \\) is associated with atomic features \\( \\mathbf{h}_i \\in \\mathbb{R}^F \\).\n   - \\( E \\) represents bonds with edges \\( \\{e_{ij}: (i, j) \\in E\\} \\), where \\( \\mathbf{d}_{ij} \\in \\mathbb{R}^3 \\) is the relative position vector between atoms \\( i \\) and \\( j \\).\n\n2. **SE(3)-Equivariance**: A function \\( f \\) is SE(3)-equivariant if, for any \\( g \\in SE(3) \\), \\( f(g \\cdot \\mathbf{x}) = g \\cdot f(\\mathbf{x}) \\).\n\n3. **Angular Features**: For atomic neighbors \\( j, k \\in \\mathcal{N}(i) \\), define angles:\n   \\[ \n   \\theta_{ijk} = \\arccos \\left( \\frac{\\mathbf{d}_{ij} \\cdot \\mathbf{d}_{ik}}{\\|\\mathbf{d}_{ij}\\| \\cdot \\|\\mathbf{d}_{ik}\\|} \\right).\n   \\]\n\n---\n\n### Methodological Features and Key Enhancements\n\n#### 1. **Geometry-Enhanced Directional Attention (GEDA)**\nThe proposed GEDA mechanism directly integrates angular and directional features into the attention computation, ensuring an expressive embedding update for both atomic and hierarchical substructural interactions.\n\n##### GEDA Attention Scores:\nFor each atom \\( i \\):\n1. Compute directional encodings \\( \\mathbf{g}_{ij} \\):\n   \\[\n   \\mathbf{g}_{ij} = \\left( \\|\\mathbf{d}_{ij}\\|, \\frac{\\mathbf{d}_{ij}}{\\|\\mathbf{d}_{ij}\\|} \\right).\n   \\]\n2. Augment \\( \\mathbf{g}_{ij} \\) with angular features \\( \\theta_{ijk} \\) for neighbors \\( j, k \\in \\mathcal{N}(i) \\):\n   \\[\n   \\mathbf{g}_{ijk}^{(\\mathrm{aug})} = (\\mathbf{g}_{ij}, \\theta_{ijk}).\n   \\]\n3. Compute attention scores \\( \\alpha_{ij} \\) using a softmax normalized by all neighbors of \\( i \\):\n   \\[\n   \\alpha_{ij} = \\frac{\\exp(\\phi(\\mathbf{h}_i, \\mathbf{h}_j, \\mathbf{g}_{ijk}^{(\\mathrm{aug})}))}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(\\phi(\\mathbf{h}_i, \\mathbf{h}_k, \\mathbf{g}_{ik}^{(\\mathrm{aug})}))},\n   \\]\n   where \\( \\phi(\\cdot) \\) is a trainable scoring function combining node features and augmented geometric encodings.\n4. Aggregate atomic features \\( \\mathbf{m}_i \\):\n   \\[\n   \\mathbf{m}_i = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\cdot \\mathbf{W}_a \\mathbf{h}_j,\n   \\]\n   where \\( \\mathbf{W}_a \\) is a learnable transformation matrix.\n\n##### Hierarchical Substructure Representation:\n1. Group atoms into functional substructures \\( \\{c_1, c_2, \\dots, c_k\\} \\) (e.g., rings, chains).\n2. Compute embedding for each substructure \\( c \\):\n   \\[\n   \\mathbf{h}_c = \\sum_{i \\in c} \\beta_i \\cdot \\mathbf{h}_i,\n   \\]\n   where \\( \\beta_i \\) are derived from hierarchical attention weights.\n\n---\n\n#### 2. **Enhanced SE(3)-Equivariance Guarantees**\nHEDGE-Net ensures full equivariance for both geometric attention and message updates:\n1. **Geometric Attention Equivariance:** The directional encoding \\( \\mathbf{g}_{ij} \\) and angular augmentation \\( \\theta_{ijk} \\) are formulated to transform consistently under SE(3). This ensures attention computation respects the symmetry properties of molecular geometries.\n2. **Message Passing Equivariance:** An updated message passing rule incorporates equivariant transformations explicitly:\n   \\[\n   \\mathbf{h}_i^{(t+1)} = \\sigma \\left( \\mathbf{W}_m \\mathbf{h}_i^{(t)} + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{W}_m^{\\prime} \\mathbf{h}_j^{(t)} \\odot \\mathbf{g}_{ij} \\right),\n   \\]\n   where \\( \\mathbf{W}_m \\) and \\( \\mathbf{W}_m^{\\prime} \\) are equivariant learnable matrices.\n\n---\n\n#### 3. **Refined Algorithmic Workflow**\n```\nAlgorithm: HEDGE-Net for SE(3)-Equivariant Molecular Modeling\nInput: Molecular graph \\( G = (V, E) \\), features \\( \\mathbf{h}_i \\), position vectors \\( \\mathbf{d}_{ij} \\).\nOutput: Energy prediction \\( E(G) \\), atomic forces \\( \\mathbf{F}_i \\).\n\n1. Initialize \\( \\mathbf{h}_i^{(0)} \\) for all nodes.\n2. For each layer \\( t = 1, \\dots, T \\):\n   a. Compute augmented geometric encodings \\( \\mathbf{g}_{ijk}^{(\\mathrm{aug})} \\).\n   b. Calculate attention weights \\( \\alpha_{ij} \\) using GEDA.\n   c. Aggregate atomic features \\( \\mathbf{m}_i \\) and update embeddings \\( \\mathbf{h}_i^{(t+1)} \\).\n3. Group nodes into substructures and compute substructural embeddings \\( \\mathbf{h}_c \\).\n4. Aggregate global features for energy prediction \\( E(G) \\):\n   \\[\n   E(G) = g\\left( \\sum_{c \\in C} \\mathbf{W}_E \\mathbf{h}_c \\right),\n   \\]\n   where \\( g(\\cdot) \\) is a differentiable pooling function.\n5. Backpropagate energy gradients to compute forces \\( \\mathbf{F}_i = -\\partial E(G)/\\partial \\mathbf{r}_i \\).\n```\n\n---\n\n### Theoretical Properties\n1. **Equivariance Proof:** All components (attention, message updates, pooling) preserve SE(3)-equivariance rigorously, as angular and directional computations are geometry-consistent.\n2. **Expressivity:** GEDA enhances representation power by incorporating fine-grained directional and angular interactions, surpassing simpler geometric attention mechanisms.\n\n---\n\n### Complexity\n- **Time Complexity:** \\( O(|V| + |E|d^2) \\), where \\( d \\) is feature dimensionality.\n- **Space Complexity:** \\( O(|V|d + |E|d) \\)."
  }